{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from hashlib import sha1\n",
        "\n",
        "class HyperLogLog:\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "        self.m = 2 ** p\n",
        "        self.registers = [0] * self.m\n",
        "        self.alpha = self.get_alpha()\n",
        "\n",
        "    def get_alpha(self):\n",
        "        if self.m == 16:\n",
        "            return 0.673\n",
        "        elif self.m == 32:\n",
        "            return 0.697\n",
        "        elif self.m == 64:\n",
        "            return 0.709\n",
        "        else:\n",
        "            return 0.7213 / (1 + 1.079 / self.m)\n",
        "\n",
        "    def add(self, item):\n",
        "        # Hash the item using SHA-1 and convert to binary\n",
        "        hash_value = sha1(str(item).encode('utf-8')).hexdigest()\n",
        "        binary_hash = bin(int(hash_value, 16))[2:].zfill(160)\n",
        "\n",
        "        # Use the first p bits to determine the register\n",
        "        register_index = int(binary_hash[:self.p], 2)\n",
        "\n",
        "        # The remaining bits determine the number of leading zeros\n",
        "        remaining_bits = binary_hash[self.p:]\n",
        "        leading_zeros = len(remaining_bits) - len(remaining_bits.lstrip('0')) + 1\n",
        "\n",
        "        # Update the register with the maximum leading zeros observed\n",
        "        self.registers[register_index] = max(self.registers[register_index], leading_zeros)\n",
        "\n",
        "    def estimate(self):\n",
        "        # Calculate the harmonic mean of 2^-register\n",
        "        indicator = sum([2 ** -r for r in self.registers])\n",
        "        return self.alpha * self.m ** 2 / indicator"
      ],
      "metadata": {
        "id": "Pp2tBlY66zGe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNFUTzT-1hTd",
        "outputId": "d65c38c2-6017-4f90-982d-0a916a12bb30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Profiles: 1000000\n",
            "Actual Unique Profiles: 1000000\n",
            "HyperLogLog Estimate of Unique Profiles: 945912.59\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "total_profiles = 1000000\n",
        "\n",
        "profile_ids = random.sample(range(0, total_profiles), total_profiles)\n",
        "\n",
        "# Initialize HyperLogLog with p=10 (m=1024 registers)\n",
        "hll = HyperLogLog(p=10)\n",
        "\n",
        "for number in profile_ids:\n",
        "  hll.add(number)\n",
        "\n",
        "actual_unique = len(set(profile_ids))\n",
        "\n",
        "hll_estimate = hll.estimate()\n",
        "\n",
        "print(f\"Total Profiles: {total_profiles}\")\n",
        "print(f\"Actual Unique Profiles: {actual_unique}\")\n",
        "print(f\"HyperLogLog Estimate of Unique Profiles: {hll_estimate:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "from pyspark.sql.functions import approx_count_distinct\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HyperLogLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"profile_id\", IntegerType(), False)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame([(num,) for num in profile_ids], schema)\n",
        "\n",
        "exact_count = df.select(\"profile_id\").distinct().count()\n",
        "print(f\"Exact Unique Profiles: {exact_count}\")\n",
        "\n",
        "approx_count = df.select(approx_count_distinct(\"profile_id\")).collect()[0][0]\n",
        "print(f\"Spark HLL Estimate of Unique Profiles: {approx_count:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMq1pFi2C_Hw",
        "outputId": "5cc3c80e-91fd-4937-80b0-d761bef5c700"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Unique Profiles: 1000000\n",
            "Spark HLL Estimate of Unique Profiles: 943039.00\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtZky1qGxBLW",
        "outputId": "a0726b8e-c4a0-4a39-e4a4-d79eace851fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.69\n",
            "Precision: 0.78\n",
            "Recall: 0.78\n",
            "F1 Score: 0.71\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ChurnPrediction\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (30, 0.5, 5, 0.1, 0), (60, 1.2, 20, 0.3, 1), (50, 1.0, 15, 0.2, 0),\n",
        "    (40, 1.5, 30, 0.5, 0), (80, 2.0, 25, 0.4, 1), (55, 1.2, 10, 0.1, 0),\n",
        "    (45, 0.8, 40, 0.2, 0), (90, 2.5, 35, 0.3, 1), (20, 0.5, 50, 0.5, 1),\n",
        "    (35, 0.2, 5, 0.1, 0), (70, 0.1, 10, 0.2, 0), (65, 0.5, 20, 0.1, 1),\n",
        "    (30, 0.3, 15, 0.3, 0), (85, 0.4, 25, 0.4, 1), (75, 0.8, 30, 0.5, 1),\n",
        "    (95, 0.7, 35, 0.2, 0), (100, 1.5, 40, 0.3, 0), (25, 1.8, 45, 0.4, 1),\n",
        "    (15, 1.0, 50, 0.5, 1), (10, 0.9, 5, 0.1, 0),\n",
        "    (50, 0.6, 10, 0.3, 0), (60, 0.4, 20, 0.2, 0), (55, 0.2, 30, 0.3, 1),\n",
        "    (45, 0.3, 40, 0.1, 0), (90, 0.5, 10, 0.4, 1), (95, 0.4, 15, 0.2, 0),\n",
        "    (40, 0.5, 20, 0.3, 1), (30, 0.7, 25, 0.4, 1), (20, 0.3, 30, 0.5, 0),\n",
        "    (10, 0.2, 35, 0.4, 1), (0, 0.1, 5, 0.1, 0), (10, 0.5, 20, 0.2, 1),\n",
        "    (20, 0.6, 15, 0.3, 0), (30, 0.7, 10, 0.1, 1), (40, 0.9, 25, 0.4, 0),\n",
        "    (50, 0.8, 35, 0.5, 1), (40, 0.4, 20, 0.3, 0), (35, 0.5, 30, 0.4, 1),\n",
        "    (30, 0.6, 40, 0.5, 0), (25, 0.7, 15, 0.1, 1), (20, 0.3, 5, 0.2, 0),\n",
        "    (15, 0.2, 10, 0.3, 0), (10, 0.1, 5, 0.1, 1), (5, 0.0, 0, 0.0, 0),\n",
        "    (100, 0.6, 10, 0.2, 0), (95, 0.7, 20, 0.3, 1), (90, 0.8, 15, 0.2, 0),\n",
        "    (85, 0.9, 25, 0.4, 1), (80, 0.8, 30, 0.5, 1), (75, 0.7, 35, 0.4, 0),\n",
        "    (70, 0.6, 40, 0.3, 0), (60, 0.5, 5, 0.1, 0), (50, 0.4, 20, 0.2, 1),\n",
        "    (40, 0.5, 15, 0.3, 0), (30, 0.6, 10, 0.1, 0), (20, 0.2, 25, 0.4, 1),\n",
        "    (10, 0.3, 30, 0.5, 1), (0, 0.1, 5, 0.2, 0), (10, 0.5, 20, 0.3, 0),\n",
        "    (20, 0.6, 15, 0.4, 1), (30, 0.7, 25, 0.5, 1), (40, 0.4, 10, 0.1, 0),\n",
        "    (50, 0.3, 5, 0.2, 0), (60, 0.2, 20, 0.3, 1), (70, 0.5, 15, 0.4, 1)\n",
        "]\n",
        "\n",
        "columns = ['Network Quality: Latency', 'Network Quality: Dropped Calls', 'Internet Usage (GB)', 'Network Quality: Data Drops', 'Churn']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['Network Quality: Latency', 'Network Quality: Dropped Calls', 'Internet Usage (GB)', 'Network Quality: Data Drops'], outputCol=\"features\")\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"Churn\", maxIter=1000)\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\", rawPredictionCol=\"prediction\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
        "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
        "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "precision = precision_evaluator.evaluate(predictions)\n",
        "recall = recall_evaluator.evaluate(predictions)\n",
        "f1 = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to improve performance via hyperparameter tuning."
      ],
      "metadata": {
        "id": "XSlkBaZkZYGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
        "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
        "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.0, 0.001, 0.01])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.5])\n",
        "             .build())\n",
        "\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=f1_evaluator,\n",
        "                    numFolds=3)\n",
        "\n",
        "cvModel = cv.fit(train_df)\n",
        "cv_predictions = cvModel.transform(test_df)\n",
        "\n",
        "cv_precision = precision_evaluator.evaluate(cv_predictions)\n",
        "cv_recall = recall_evaluator.evaluate(cv_predictions)\n",
        "cv_f1 = f1_evaluator.evaluate(cv_predictions)\n",
        "\n",
        "print(\"Tuned Model Performance:\")\n",
        "print(f'Precision: {cv_precision:.2f}')\n",
        "print(f'Recall: {cv_recall:.2f}')\n",
        "print(f'F1 Score: {cv_f1:.2f}')\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(cvModel.bestModel.stages[-1].extractParamMap())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc51BHQVdTTY",
        "outputId": "f60ebe0c-e139-40b2-e2f9-6c78bf5a8099"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Model Performance:\n",
            "Precision: 0.78\n",
            "Recall: 0.78\n",
            "F1 Score: 0.71\n",
            "Best Model Parameters:\n",
            "{Param(parent='LogisticRegression_c411e91fe222', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_c411e91fe222', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_c411e91fe222', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_c411e91fe222', name='featuresCol', doc='features column name.'): 'scaled_features', Param(parent='LogisticRegression_c411e91fe222', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_c411e91fe222', name='labelCol', doc='label column name.'): 'Churn', Param(parent='LogisticRegression_c411e91fe222', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_c411e91fe222', name='maxIter', doc='max number of iterations (>= 0).'): 1000, Param(parent='LogisticRegression_c411e91fe222', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_c411e91fe222', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_c411e91fe222', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_c411e91fe222', name='regParam', doc='regularization parameter (>= 0).'): 0.0, Param(parent='LogisticRegression_c411e91fe222', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_c411e91fe222', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_c411e91fe222', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
          ]
        }
      ]
    }
  ]
}